{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import config as conf\n",
    "\n",
    "from data.mnist import Mnist\n",
    "from models.vanilla_gan import VanillaGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1,  Generator Loss: 2.132909,  Discriminator Loss: 0.504999\n",
      "Epoch: 2,  Generator Loss: 2.900801,  Discriminator Loss: 0.366182\n",
      "Epoch: 3,  Generator Loss: 3.692217,  Discriminator Loss: 0.272288\n",
      "Epoch: 4,  Generator Loss: 3.159725,  Discriminator Loss: 0.387419\n",
      "Epoch: 5,  Generator Loss: 2.804310,  Discriminator Loss: 0.463262\n",
      "Epoch: 6,  Generator Loss: 2.733387,  Discriminator Loss: 0.471337\n",
      "Epoch: 7,  Generator Loss: 2.527001,  Discriminator Loss: 0.546305\n",
      "Epoch: 8,  Generator Loss: 2.414601,  Discriminator Loss: 0.584552\n",
      "Epoch: 9,  Generator Loss: 2.265661,  Discriminator Loss: 0.654234\n",
      "Epoch: 10,  Generator Loss: 2.072481,  Discriminator Loss: 0.718253\n",
      "Epoch: 11,  Generator Loss: 1.893335,  Discriminator Loss: 0.772802\n",
      "Epoch: 12,  Generator Loss: 1.801005,  Discriminator Loss: 0.809294\n",
      "Epoch: 13,  Generator Loss: 1.736680,  Discriminator Loss: 0.820199\n",
      "Epoch: 14,  Generator Loss: 1.681808,  Discriminator Loss: 0.846175\n",
      "Epoch: 15,  Generator Loss: 1.649302,  Discriminator Loss: 0.857915\n",
      "Epoch: 16,  Generator Loss: 1.689942,  Discriminator Loss: 0.829585\n",
      "Epoch: 17,  Generator Loss: 1.696904,  Discriminator Loss: 0.824202\n",
      "Epoch: 18,  Generator Loss: 1.706363,  Discriminator Loss: 0.827919\n",
      "Epoch: 19,  Generator Loss: 1.646207,  Discriminator Loss: 0.853357\n",
      "Epoch: 20,  Generator Loss: 1.665893,  Discriminator Loss: 0.846458\n",
      "Epoch: 21,  Generator Loss: 1.658101,  Discriminator Loss: 0.851011\n",
      "Epoch: 22,  Generator Loss: 1.627541,  Discriminator Loss: 0.862327\n",
      "Epoch: 23,  Generator Loss: 1.649304,  Discriminator Loss: 0.852231\n",
      "Epoch: 24,  Generator Loss: 1.593382,  Discriminator Loss: 0.870481\n",
      "Epoch: 25,  Generator Loss: 1.588238,  Discriminator Loss: 0.875894\n",
      "Epoch: 26,  Generator Loss: 1.601368,  Discriminator Loss: 0.866319\n",
      "Epoch: 27,  Generator Loss: 1.585456,  Discriminator Loss: 0.877249\n",
      "Epoch: 28,  Generator Loss: 1.578508,  Discriminator Loss: 0.878943\n",
      "Epoch: 29,  Generator Loss: 1.563791,  Discriminator Loss: 0.883261\n",
      "Epoch: 30,  Generator Loss: 1.576441,  Discriminator Loss: 0.883739\n",
      "Epoch: 31,  Generator Loss: 1.567910,  Discriminator Loss: 0.889894\n",
      "Epoch: 32,  Generator Loss: 1.581282,  Discriminator Loss: 0.881486\n",
      "Epoch: 33,  Generator Loss: 1.571748,  Discriminator Loss: 0.885514\n",
      "Epoch: 34,  Generator Loss: 1.558995,  Discriminator Loss: 0.884640\n",
      "Epoch: 35,  Generator Loss: 1.562773,  Discriminator Loss: 0.885161\n",
      "Epoch: 36,  Generator Loss: 1.575207,  Discriminator Loss: 0.878519\n",
      "Epoch: 37,  Generator Loss: 1.561229,  Discriminator Loss: 0.887107\n",
      "Epoch: 38,  Generator Loss: 1.567903,  Discriminator Loss: 0.881609\n",
      "Epoch: 39,  Generator Loss: 1.567254,  Discriminator Loss: 0.877748\n",
      "Epoch: 40,  Generator Loss: 1.580422,  Discriminator Loss: 0.877391\n",
      "Epoch: 41,  Generator Loss: 1.571972,  Discriminator Loss: 0.881408\n",
      "Epoch: 42,  Generator Loss: 1.558643,  Discriminator Loss: 0.886416\n",
      "Epoch: 43,  Generator Loss: 1.563211,  Discriminator Loss: 0.885119\n",
      "Epoch: 44,  Generator Loss: 1.544765,  Discriminator Loss: 0.888243\n",
      "Epoch: 45,  Generator Loss: 1.567865,  Discriminator Loss: 0.878715\n",
      "Epoch: 46,  Generator Loss: 1.552205,  Discriminator Loss: 0.886783\n",
      "Epoch: 47,  Generator Loss: 1.568260,  Discriminator Loss: 0.882800\n",
      "Epoch: 48,  Generator Loss: 1.560477,  Discriminator Loss: 0.880765\n",
      "Epoch: 49,  Generator Loss: 1.569501,  Discriminator Loss: 0.876162\n",
      "Epoch: 50,  Generator Loss: 1.571864,  Discriminator Loss: 0.878019\n",
      "Epoch: 51,  Generator Loss: 1.557314,  Discriminator Loss: 0.880926\n",
      "Epoch: 52,  Generator Loss: 1.572319,  Discriminator Loss: 0.877940\n",
      "Epoch: 53,  Generator Loss: 1.577437,  Discriminator Loss: 0.871553\n",
      "Epoch: 54,  Generator Loss: 1.579049,  Discriminator Loss: 0.875405\n",
      "Epoch: 55,  Generator Loss: 1.555090,  Discriminator Loss: 0.883931\n",
      "Epoch: 56,  Generator Loss: 1.588286,  Discriminator Loss: 0.870727\n",
      "Epoch: 57,  Generator Loss: 1.587993,  Discriminator Loss: 0.871401\n",
      "Epoch: 58,  Generator Loss: 1.581541,  Discriminator Loss: 0.871119\n",
      "Epoch: 59,  Generator Loss: 1.589736,  Discriminator Loss: 0.868571\n",
      "Epoch: 60,  Generator Loss: 1.591915,  Discriminator Loss: 0.868235\n",
      "Epoch: 61,  Generator Loss: 1.603588,  Discriminator Loss: 0.864006\n",
      "Epoch: 62,  Generator Loss: 1.608123,  Discriminator Loss: 0.863969\n",
      "Epoch: 63,  Generator Loss: 1.612729,  Discriminator Loss: 0.859926\n",
      "Epoch: 64,  Generator Loss: 1.607894,  Discriminator Loss: 0.861147\n",
      "Epoch: 65,  Generator Loss: 1.607674,  Discriminator Loss: 0.859806\n",
      "Epoch: 66,  Generator Loss: 1.614531,  Discriminator Loss: 0.857477\n",
      "Epoch: 67,  Generator Loss: 1.607002,  Discriminator Loss: 0.864378\n",
      "Epoch: 68,  Generator Loss: 1.608429,  Discriminator Loss: 0.859976\n",
      "Epoch: 69,  Generator Loss: 1.622644,  Discriminator Loss: 0.856315\n",
      "Epoch: 70,  Generator Loss: 1.622952,  Discriminator Loss: 0.852338\n",
      "Epoch: 71,  Generator Loss: 1.630731,  Discriminator Loss: 0.848220\n",
      "Epoch: 72,  Generator Loss: 1.636284,  Discriminator Loss: 0.847334\n",
      "Epoch: 73,  Generator Loss: 1.642064,  Discriminator Loss: 0.849527\n",
      "Epoch: 74,  Generator Loss: 1.651602,  Discriminator Loss: 0.846417\n",
      "Epoch: 75,  Generator Loss: 1.649447,  Discriminator Loss: 0.842578\n",
      "Epoch: 76,  Generator Loss: 1.652366,  Discriminator Loss: 0.840410\n",
      "Epoch: 77,  Generator Loss: 1.652797,  Discriminator Loss: 0.839418\n",
      "Epoch: 78,  Generator Loss: 1.660858,  Discriminator Loss: 0.838712\n",
      "Epoch: 79,  Generator Loss: 1.648013,  Discriminator Loss: 0.844764\n",
      "Epoch: 80,  Generator Loss: 1.663623,  Discriminator Loss: 0.831012\n",
      "Epoch: 81,  Generator Loss: 1.659656,  Discriminator Loss: 0.837703\n",
      "Epoch: 82,  Generator Loss: 1.677022,  Discriminator Loss: 0.831779\n",
      "Epoch: 83,  Generator Loss: 1.656724,  Discriminator Loss: 0.837705\n",
      "Epoch: 84,  Generator Loss: 1.672121,  Discriminator Loss: 0.828935\n",
      "Epoch: 85,  Generator Loss: 1.669044,  Discriminator Loss: 0.833218\n",
      "Epoch: 86,  Generator Loss: 1.675771,  Discriminator Loss: 0.832036\n",
      "Epoch: 87,  Generator Loss: 1.684987,  Discriminator Loss: 0.823249\n",
      "Epoch: 88,  Generator Loss: 1.671390,  Discriminator Loss: 0.826860\n",
      "Epoch: 89,  Generator Loss: 1.691385,  Discriminator Loss: 0.818323\n",
      "Epoch: 90,  Generator Loss: 1.685722,  Discriminator Loss: 0.819578\n",
      "Epoch: 91,  Generator Loss: 1.702273,  Discriminator Loss: 0.812302\n",
      "Epoch: 92,  Generator Loss: 1.689039,  Discriminator Loss: 0.815417\n",
      "Epoch: 93,  Generator Loss: 1.707455,  Discriminator Loss: 0.805340\n",
      "Epoch: 94,  Generator Loss: 1.707689,  Discriminator Loss: 0.809909\n",
      "Epoch: 95,  Generator Loss: 1.706176,  Discriminator Loss: 0.807645\n",
      "Epoch: 96,  Generator Loss: 1.719406,  Discriminator Loss: 0.806759\n",
      "Epoch: 97,  Generator Loss: 1.710232,  Discriminator Loss: 0.809159\n",
      "Epoch: 98,  Generator Loss: 1.712460,  Discriminator Loss: 0.798214\n",
      "Epoch: 99,  Generator Loss: 1.724976,  Discriminator Loss: 0.798809\n",
      "Epoch: 100,  Generator Loss: 1.730549,  Discriminator Loss: 0.798665\n"
     ]
    }
   ],
   "source": [
    "def feature_normalize(features):\n",
    "    return (features - 0.5) / 0.5\n",
    "\n",
    "\n",
    "def feature_denormalize(features):\n",
    "    return (features + 1) / 2\n",
    "\n",
    "\n",
    "def main():\n",
    "    model_spec_name = \"%s-model-spec.json\" % conf.MODEL_NAME\n",
    "    model_rslt_name = \"%s-results.pickle\" % conf.MODEL_NAME\n",
    "\n",
    "    model_save_path = os.path.join(conf.MODEL_SAVE_DIR, conf.MODEL_NAME)\n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "    \n",
    "    model_ckpt_path = os.path.join(model_save_path, \"model-ckpt\")\n",
    "    model_spec_path = os.path.join(model_save_path, model_spec_name)\n",
    "    model_rslt_path = os.path.join(model_save_path, model_rslt_name)\n",
    "\n",
    "    loader = Mnist()\n",
    "\n",
    "    features = np.vstack([loader.train_features, loader.test_features])\n",
    "    features = feature_normalize(features).astype(np.float32)\n",
    "\n",
    "    num_sets = loader.num_train_sets + loader.num_test_sets\n",
    "    \n",
    "    feature_depth = loader.feature_depth\n",
    "    feature_shape = loader.feature_shape\n",
    "\n",
    "    latent_depth = conf.LATENT_DEPTH\n",
    "\n",
    "    batch_size = conf.BATCH_SIZE\n",
    "    num_epochs = conf.NUM_EPOCHS\n",
    "\n",
    "    model = VanillaGAN(feature_depth)\n",
    "\n",
    "    generator_opt = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "    discriminator_opt = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(x, z):\n",
    "        with tf.GradientTape() as generator_tape, tf.GradientTape() as discriminator_tape:\n",
    "            generator_loss = model.generator_loss(z)\n",
    "            discriminator_loss = model.discriminator_loss(x, z)\n",
    "\n",
    "            grads_generator_loss = generator_tape.gradient(\n",
    "                target=generator_loss, sources=model.generator.trainable_variables\n",
    "            )\n",
    "            grads_discriminator_loss = discriminator_tape.gradient(\n",
    "                target=discriminator_loss, sources=model.discriminator.trainable_variables\n",
    "            )\n",
    "\n",
    "            discriminator_opt.apply_gradients(\n",
    "                zip(grads_discriminator_loss, model.discriminator.trainable_variables)\n",
    "            )\n",
    "            generator_opt.apply_gradients(\n",
    "                zip(grads_generator_loss, model.generator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        return generator_loss, discriminator_loss\n",
    "\n",
    "    ckpt = tf.train.Checkpoint(generator=model.generator, discriminator=model.discriminator)\n",
    "\n",
    "    steps_per_epoch = num_sets // batch_size\n",
    "    train_steps = steps_per_epoch * num_epochs\n",
    "\n",
    "    generator_losses = []\n",
    "    discriminator_losses = []\n",
    "    generator_losses_epoch = []\n",
    "    discriminator_losses_epoch = []\n",
    "    x_fakes = []\n",
    "    for i in range(1, train_steps+1):\n",
    "        epoch = i // steps_per_epoch\n",
    "\n",
    "        idxes = np.random.choice(num_sets, batch_size, replace=False)\n",
    "        x_i = features[idxes]\n",
    "        z_i = np.random.normal(size=[batch_size, latent_depth]).astype(np.float32)\n",
    "\n",
    "        generator_loss_i, discriminator_loss_i = train_step(x_i, z_i)\n",
    "        \n",
    "        generator_losses.append(generator_loss_i)\n",
    "        discriminator_losses.append(discriminator_loss_i)\n",
    "\n",
    "        if i % steps_per_epoch == 0:\n",
    "            x_fake = model.generator(z_i, training=False)\n",
    "            x_fake = feature_denormalize(x_fake)\n",
    "\n",
    "            generator_loss_epoch = np.mean(generator_losses[-steps_per_epoch:])\n",
    "            discriminator_loss_epoch = np.mean(discriminator_losses[-steps_per_epoch:])\n",
    "\n",
    "            print(\"Epoch: %i,  Generator Loss: %f,  Discriminator Loss: %f\" % \\\n",
    "                (epoch, generator_loss_epoch, discriminator_loss_epoch)\n",
    "            )\n",
    "\n",
    "            generator_losses_epoch.append(generator_loss_epoch)\n",
    "            discriminator_losses_epoch.append(discriminator_loss_epoch)\n",
    "\n",
    "            x_fakes.append(x_fake)\n",
    "            \n",
    "            ckpt.save(file_prefix=model_ckpt_path)\n",
    "\n",
    "            with open(model_rslt_path, \"wb\") as f:\n",
    "                pickle.dump((generator_losses_epoch, discriminator_losses_epoch, x_fakes), f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%sudo` not found.\n"
     ]
    }
   ],
   "source": [
    "%sudo pip3 install --upgrade tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
